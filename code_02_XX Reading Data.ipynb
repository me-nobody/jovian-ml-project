{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this module, we look at ways to read and analyze data in files.\n",
    "\n",
    "## 02_01 Reading Raw Files\n",
    "\n",
    "Python supports a number of standard and custom libraries to read files of all types into python variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data read from file :  In order to construct data pipelines and networks that stream, process, and store data, data engineers and data-science DevOps specialists must understand how to combine multiple big data technologies\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "#Read the file using standard python libaries\n",
    "with open(os.getcwd()+ \"/Spark-Course-Description.txt\", 'r') as fh:  \n",
    "    filedata = fh.read()\n",
    "    \n",
    "#Print first 200 characters in the file\n",
    "print(\"Data read from file : \", filedata[0:200] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02_02 Reading using NLTK CorpusReader\n",
    "\n",
    "Read the same text file using a Corpus Reader\n",
    "\n",
    "NLTK supports multiple CorpusReaders depending upon the type of data source. Details available in http://www.nltk.org/howto/corpus.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In order to construct data pipelines and networks that stream, process, and store data, data engineers and data-science DevOps specialists must understand how to combine multiple big data technologies. In this course, discover how to build big data pipelines around Apache Spark. Join Kumaran Ponnambalam as he takes you through how to make Apache Spark work with other big data technologies. He covers the basics of Apache Kafka Connect and how to integrate it with Spark for real-time streaming. In addition, he demonstrates how to use the various technologies to construct an end-to-end project that solves a real-world business problem.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#install nltk from anaconda prompt using \"pip install nltk\"\n",
    "import nltk\n",
    "#Download punkt package, used part of the other commands\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk.corpus.reader.plaintext import PlaintextCorpusReader\n",
    "\n",
    "#Read the file into a corpus. The same command can read an entire directory\n",
    "corpus=PlaintextCorpusReader(os.getcwd(),\"Spark-Course-Description.txt\")\n",
    "\n",
    "#Print raw contents of the corpus\n",
    "print(corpus.raw())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02_03 Exploring the Corpus\n",
    "\n",
    "The corpus library supports a number of functions to extract words, paragraphs and sentences from the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in this corpus :  ['Spark-Course-Description.txt']\n",
      "\n",
      " Total paragraphs in this corpus :  1\n",
      "\n",
      " Total sentences in this corpus :  5\n",
      "\n",
      " The first sentence :  ['In', 'order', 'to', 'construct', 'data', 'pipelines', 'and', 'networks', 'that', 'stream', ',', 'process', ',', 'and', 'store', 'data', ',', 'data', 'engineers', 'and', 'data', '-', 'science', 'DevOps', 'specialists', 'must', 'understand', 'how', 'to', 'combine', 'multiple', 'big', 'data', 'technologies', '.']\n",
      "\n",
      " Words in this corpus :  ['In', 'order', 'to', 'construct', 'data', 'pipelines', ...]\n"
     ]
    }
   ],
   "source": [
    "#Extract the file IDs from the corpus\n",
    "print(\"Files in this corpus : \", corpus.fileids())\n",
    "\n",
    "#Extract paragraphs from the corpus\n",
    "paragraphs=corpus.paras()\n",
    "print(\"\\n Total paragraphs in this corpus : \", len(paragraphs))\n",
    "\n",
    "#Extract sentences from the corpus\n",
    "sentences=corpus.sents()\n",
    "print(\"\\n Total sentences in this corpus : \", len(sentences))\n",
    "print(\"\\n The first sentence : \", sentences[0])\n",
    "\n",
    "#Extract words from the corpus\n",
    "print(\"\\n Words in this corpus : \",corpus.words() )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02_04 Analyze the Corpus\n",
    "\n",
    "The NLTK library provides a number of functions to analyze the distributions and aggregates for data in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words in the corpus :  [('to', 8), ('data', 7), (',', 5), ('-', 5), ('how', 5), ('.', 5), ('and', 4), ('In', 3), ('big', 3), ('technologies', 3)]\n",
      "\n",
      " Distribution for \"Spark\" :  3\n"
     ]
    }
   ],
   "source": [
    "#Find the frequency distribution of words in the corpus\n",
    "course_freq_dist=nltk.FreqDist(corpus.words())\n",
    "\n",
    "#Print most commonly used words\n",
    "print(\"Top 10 words in the corpus : \", course_freq_dist.most_common(10))\n",
    "\n",
    "#find the distribution for a specific word\n",
    "print(\"\\n Distribution for \\\"Spark\\\" : \",course_freq_dist.get(\"Spark\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
